\chapter{Background}
\label{chap:background}

This research is settled in the context of the meSch (Material EncounterS with digital Cultural Heritage) EU project. Furthermore, the Visual Editor for Interactive Installations (VEII toolkit) which was developed during the implementation part of this thesis is tightly connected to a middleware for Smart Spaces called meSchup platform. This chapter provides an overview of the meSch EU project as well as the meSchup platform and its connection to the Visual Editor for Interactive Installations.

\section{MeSch EU Project}
\label{sec:meschproject}

The meSch project is coordinated by the Sheffield Hallam University and is a cooperation of twelve different partners of six European countries which started working together in February 2013. The project is funded by the European Communityâ€™s Seventh Framework Program "ICT for access to cultural resources" and will last for four years. The goal of the project is to contribute new ways to connect physical dimensions of museum exhibitions with digital cross-media information. Therefore, the participants of the meSch project design, develop and deploy tools which enable the creation of tangible interactive experiences. One of these tools is a platform which enables Cultural Heritage Professionals (CHPs) to create smart objects and intelligent environments and even compose embedded digital content without any technical expertise. Smart objects are objects that support the interaction with other smart objects as well as with people and the physical world. Intelligent environments are spaces which often contain embedded systems as well as different information and communication technology to enhance visitors experiences by creating interactive spaces through combining the digital world with the physical world. The meSch project follows the principle of co-design where designers, developers and stake-holders are equal partners in the process of creation and evaluation. The real-world evaluation of meSch technology takes place in three large-scale test beds within different museums in collaboration with the public and cultural heritage stakeholder\footnote{Official meSch EU Website, \url{http://mesch-project.eu/} (last accessed on \today)}. 

\section{MeSchup Platform}
\label{sec:meschplatform}

The VEII toolkit is based on the meSchup platform which is an external component build in the meSch EU project and the University of Stuttgart. One output of the meSch EU project is the meSchup platform which opens up the creation of smart environments to non-experts. Smart environments are settings in which technical elements like sensors, actuators, displays or other computational components where integrated seamlessly in everyday objects. The meSchup platform is build with a master-slave architecture consisting of a server- and multiple client-software and hardware components \cite{Kubitza2015n}. 

\textbf{1. Server-software:}
\newline
The server is running on a central computer node providing a web-based user interface. There users can configure devices and create behavior rules written in Javascript. Additionally, the server software provides a rule engine that triggers events and runs user-generated scripts, so called behavior rules, whenever sensor data from a configured device is received. Thereby smart behavior is realized based on the behavior rules that turns sensor events into actuator executions. 

\textbf{2. Client-software:}
\newline
For each end device which is supported by the meSchup platform experts implement a platform specific firmware. Through this firmware the meSchup platform allows the abstraction from the device specific operation system as well as its hardware or communication technology and protocol. Users of the meSchup platform just need to install the firmware once on their devices to get control over its abilities. Each device with the firmware installed can be configured via the web interface of the server-software component.

The VEII toolkit communicates with the meSchup platform via a REST API. The VEII toolkit targets at providing an easy and understandable way to create behavior rules for non-programmers. Therefore, the toolkit supports non-programmers to create interactive digital content by using the content created within the VEII Slide Editor and combining it with behavior rules of the meSchup platform.

\chapter{Related Work}
\label{chap:relwork}
%\vspace{-3cm}
%\vspace{2cm}

This thesis is inspired by former research and to understand the state of the current research, this chapter discusses related work about ubiquitous displays and interactive installations.

\section{Ubiquitous Displays}
\label{sec:ud}
There exist many different and interesting approaches for ubiquitous displays in the literature. The Everywhere Display creates a ubiquitous graphical interface through a LCD/DLP projector supported by a motorized and computer-controlled pan-tilt mirror. Thus, the Everywhere Display \cite{Pinhanez2001a} is able to transform any surface into a projected screen whereby it is also able to automatically correct distortion by using its rotating mirror. The developers planned to add gesture recognition of hands in the future to make the projected screen interactive. This approach would eliminate the need to carry a computer or mobile device with you. Users could request a computer display just by using a specific gesture to an overhead camera which would display the projected image on a surface near the users' location \cite{Pinhanez2001a}.
In \cite{Sukaviriya2003} the Everywhere Display is used to create an interactive prototype in a retail environment where the product information are displayed in situation. Within this prototype user interaction is provided by gesture recognition whereby the system processes the video signals from an installed camera in combination with the projector of the Everywhere Display. In this case the installation was used to display further or detailed information of a specific product (e.g. a size chart for pants) but there are also other scenarios mentioned like using the projection as a guidance system through the retail store.

In a different approach for ubiquitous displays using projectors the researchers implemented two different features. Firstly an automatic scene modeling of a dynamically changing indoor environment. Secondly an automatic selection of the surface on which the content is displayed. The main advantage of this approach is that it can automatically cope with a dynamically changing environment. For the modeling of the scene a coarse-to-fine approach was chosen to achieve a good trade-off between efficiency and accuracy of the modeling process. First there is a core scanning of the scene with a range scanner. Second there is a fine scanning for projection areas through a combination of a projector and  color camera. A projection surface is appropriate if it is planar, uniformly-colored with a diffuse surface, low saturation and high brightness \cite{Tokuda2003a}. 

%\cite{Gehring2012a}

%\cite{Boring2011}

An innovative approach of using ubiquitous displays in form of an interactive display robot is introduced in \cite{Choi2013}. Usually devices like projectors and especially sensors are set up in a specific place and are not mobile anymore. The wheeled interactive display robot is equipped with a small projector which is mounted on a pan/tilt system which enables the possibility to choose projection position freely. So the robot opens up the usage of new display concepts as for example the possibility to create projector based display installations which cover more than one room. Furthermore, users can choose the projection position of the interactive display robot on their own by using gestures recognized by a user interface based on sensors attached to the robot. 

\cite{Campbell2014} introduces the concept of smart wallpapers. Unfortunately no real wallpaper was developed but it was investigated how people would interact with smart wallpapers as well as how the wallpapers would enrich our everyday life. Therefore, a prototype was build which consists of large projectors which project the content on the wall and a web based application. The central web server stores all the information of the different walls and provides the information for every browser which registers with the server. Gesture based interaction as well as integration into smart home system where proposed as interaction techniques.

However, those approaches are missing the opportunity of creating content for non technical experts and are often restricted to the attached sensors and actuators, so some of them could work together with the VEII toolkit in a mutually supportive way. 
%recab of all related work
The Everywhere Displays' feature of correcting distortion of displayed content by using a pan/tilt mirror on which the information is reflected \cite{Pinhanez2001a} can be replaced by the VEII toolkits' feature of correcting distortion. The toolkit provides the possibility to correct distortion by correcting the translation of the content itself and not the translation of the projected display. Furthermore, the toolkit enables users to utilize a mobile devices' accelerometer to correct distortion by tilting device accordingly.
The feature to detect whether the displaying surface is blocked or not introduced in \cite{Tokuda2003a} would enhance the toolkits' possibilities to create interactive installations in dynamically changing environments. By now the system can cope with distortion but not with objects or persons standing in front of the projector.
The sensors and actuators of the interactive display robot introduced in \cite{Choi2013} could be integrated in the toolkits' environment whereby developers and designer could access them by using behavior rules. Furthermore, the mobility of the content display robot together with the VEII toolkit opens up new ways to display projected content. So non technical experts are enabled to create location-less dynamic interactive installations with gesture recognition. In \cite{Campbell2014} it is especially mentioned that everyone's rooms are different and the system therefore needs to automatically layout its content on the smart wallpaper. They proposed a mix of algorithm and manual content positioning. The VEII toolkit could easily support the user to position and modify the displayed content on the wallpaper especially because both approaches are using the same web technologies.

\section{Interactivity through sensors and actuators}
In \cite{Benavides2014} a toolkit for creation of smart objects and environments is introduced. By using a wearable device (bracelet) enhanced with RFID technology to detect the object or surface which is touched, users are enabled to integrate every day objects into their smart environment through RFID tag stickers. After first registering an RFID tag the user has to define a rule within a mobile application about the behavior of the bracelet if it detects this specific RFID tag. Then the defined rules get executed by interacting with the object in his environment.

\cite{Kubitza2013a} targets with the problem that existing hardware (especially smartphones and tablet devices) is hard to extend with additional sensors and actuators. A method is proposed where external hardware can be connected to the devices by clipping a WebClip to its touch screen. The bi-directional communication between the WebClip and the device is managed by a webpage displayed on the device. On the one hand the communication from WebClip to the device is realized using simulated touch gestures on the other hand the communication from the device to WebClip uses light sensors which receive light sequences generated by the webpage.

The first approach introduced in \cite{Benavides2014} is some how similar to the purpose of the VEII toolkit. Regular users are enabled to create smart environments and interact with tangible object without technical expertise. Users can also create interactive behavior on their own. The second approach introduced in \cite{Kubitza2013a} could enhance the VEII toolkit and could support user to save costs by being able to enhance existing hardware with new sensors.

\section{Content delivery}
In \cite{Styliaras2007} a framework supporting a web-based multimedia presentation of a museum is introduced. With this framework curators can generate multimedia presentations on fixed information stations or web sites without any technical expertise. The developers of the framework want to minor the gap between traditional and interactive information media so the systems provides an installation wizard-like procedure which curators just need to follow to generate a new presentation. The framework separates the created content from each specific presentation to make content reusable for other installations. Therefore the framework provides a content management system for multilingual content where curators can create, organize and administer multimedia content as well as prepare the execution of the presentation application by using predefined application templates. Content creators however need to use external programs to create and adapt multimedia content like Microsoft Word or Adobe Photoshop. After finishing the content creation process curators need to export the presentation from the system and deploy it manually on the target information station. There are two types of information stations: Firstly a basic information station which is a simple touch screen monitor integrated in a bookstand. Secondly a basic information station extended by an active projection area which provides interactivity by recognizing touch gestures via a camera. Curators need to create presentations for both information stations by using the predefined application templates provided.

\cite{Alt2011} proposes an approach to use mobile devices to transfer and get content from public displays. The "Digified"-system consists of four main components. Firstly the Digifieds Server which is the main component for the data management and storage. Furthermore, it provides an RESTful API to provide access for the different clients of the system. Secondly the Digified Display Client is running on the public displays and shows the content delivered via responses on AJAX requests. Thirdly the Digified Mobile Phone client enables users to create content directly on their mobile phone and afterwards simply deploy it by using a 5-digit code or a QR-code. Last the Digified Web Client is for people without a smartphone or prefer desktop computers. The client has two main purposes which are firstly to provide detailed information about the Digifieds platform and secondly to enable users the same features as using their mobile phone.

\cite{Clinch2014} introduces the approach of using an App-Store for interactive public displays. Therefore the public displays are within a display network and therefore connected to the Mercury App-Store.  Developers are able to create and register applications at Mercury so display owners can deploy them on their devices. Furthermore, display owners are able to manage and control their devices via Mercury. Display owners and developers are therefore enabled to deliver dynamic or static content to the displays in form of simple media slideshows with images or videos as well as web-based applications.

All approaches indeed enable the user to deliver content on an interactive public display. However the VEII toolkit is more versatile and can be used in a variety of different fields while the introduced approaches in \cite{Styliaras2007}, \cite{Alt2011} and \cite{Clinch2014} are limited to a specific use case.
